# Stage 1: Export requirements
FROM python:3.11-slim AS requirements-stage

WORKDIR /tmp

RUN pip install poetry && \
    poetry self add poetry-plugin-export

COPY pyproject.toml poetry.lock* /tmp/

# Export only the necessary dependencies for ML and Celery
RUN poetry export -f requirements.txt --output requirements.txt --without-hashes --with api --with celery --with ml --with scraper

# Stage 2: Build the Celery worker
FROM python:3.11-slim

WORKDIR /code

# Copy modules needed for ML tasks
COPY /graildient_descent ./graildient_descent
COPY /data_collection ./data_collection
COPY /celery_tasks ./celery_tasks
COPY --from=requirements-stage /tmp/requirements.txt /code/requirements.txt

# Set environment variables
ENV PYTHONPATH=/code \
    S3_MODEL_PATH="" \
    S3_MODELS_BUCKET="graildient-models" \
    CELERY_BROKER_URL="amqp://guest:guest@rabbitmq:5672//" \
    CELERY_RESULT_BACKEND="rpc://"

# Install dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends gcc && \
    pip install --no-cache-dir --upgrade pip setuptools wheel && \
    pip install --no-cache-dir --upgrade -r requirements.txt && \
    apt-get remove -y gcc && \
    apt-get autoremove -y && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Command to run the Celery worker (will be overridden in docker-compose)
CMD ["celery", "-A", "celery_tasks.worker", "worker", "--loglevel=info"]
